@INPROCEEDINGS{5463719,
author={Harman, M. and Yue Jia and Langdon, W.B.},
booktitle={Software Testing, Verification, and Validation Workshops (ICSTW), 2010 Third International Conference on},
title={A Manifesto for Higher Order Mutation Testing},
year={2010},
pages={80-89},
abstract={We argue that higher order mutants are potentially better able to simulate real faults and to reveal insights into bugs than the restricted class of first order mutants. The Mutation Testing community has previously shied away from Higher Order Mutation Testing believing it to be too expensive and therefore impractical. However, this paper argues that Search Based Software Engineering can provide a solution to this apparent problem, citing results from recent work on search based optimization techniques for constructing higher order mutants. We also present a research agenda for the development of Higher Order Mutation Testing.},
keywords={optimisation;program testing;search problems;software engineering;higher order mutation testing;real faults;search based optimization technique;search based software engineering;Computer bugs;Educational institutions;Fault detection;Genetic mutations;Helium;Software engineering;Software testing;Higher Order Mutation Testing},
doi={10.1109/ICSTW.2010.13},}
Mutation Testing, Higher Order Mutants
illustrate why Higher Order Mutants are valuable, Yue and Mark's paper.

@ARTICLE{5487526,
author={Yue Jia and Harman, M.},
journal={Software Engineering, IEEE Transactions on},
title={An Analysis and Survey of the Development of Mutation Testing},
year={2011},
volume={37},
number={5},
pages={649-678},
abstract={Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest.},
keywords={fault diagnosis;program testing;comprehensive analysis;development trend analysis;empirical results;fault-based software testing technique;mutation testing development;mutation testing technique;mutation testing tool;Automata;Books;Computer languages;Educational institutions;Fault detection;Genetic mutations;History;Java;Programming profession;Software testing;Mutation testing;survey.},
doi={10.1109/TSE.2010.62},
ISSN={0098-5589},}
Survey, Mutation Testing
a review of Mutation Testing, Yue's paper. very good review.

@techreport{agrawal1989design,
  title={Design of mutant operators for the C programming language},
  author={Agrawal, Hiralal and Demillo, R and Hathaway, R\_ and Hsu, William and Hsu, Wynne and Krauser, E and Martin, Rhonda J and Mathur, A and Spafford, Eugene},
  year={1989},
  institution={Technical Report SERC-TR-41-P, Software Engineering Research Center, Purdue University}
}
mutation operators
introduction to the set of C Mutation Operators which is applied on Milu.

@inproceedings{Offutt:1993:EES:257572.257597,
 author = {Offutt, A. Jefferson and Rothermel, Gregg and Zapf, Christian},
 title = {An Experimental Evaluation of Selective Mutation},
 booktitle = {Proceedings of the 15th International Conference on Software Engineering},
 series = {ICSE '93},
 year = {1993},
 isbn = {0-89791-588-7},
 location = {Baltimore, Maryland, USA},
 pages = {100--107},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=257572.257597},
 acmid = {257597},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
}

@INPROCEEDINGS{4670308,
author={Yue Jia and Harman, M.},
booktitle={Practice and Research Techniques, 2008. TAIC PART '08. Testing: Academic Industrial Conference},
title={MILU: A Customizable, Runtime-Optimized Higher Order Mutation Testing Tool for the Full C Language},
year={2008},
pages={94-98},
abstract={This paper introduces MILU, a C mutation testing tool designed for both first order and higher order mutation testing. All previous mutation testing tools apply all possible mutation operators to the program under test. By contrast, MILU allows customization of the set of mutation operators to be applied. To reduce runtime cost, MILU uses a novel 'test harness' technique to embed mutants and their associated test sets into a single-invocation procedure.},
keywords={C language;program testing;software tools;C language;MiLu;mutation operator;runtime-optimized higher order mutation testing tool;single-invocation procedure;test harness technique;Animals;Costs;Educational institutions;Genetic mutations;Horses;Runtime;Testing;MILU;mutation testing tool},
doi={10.1109/TAIC-PART.2008.18},}
MILU, mutation testing
introduction to the tool Milu, yue's paper

@ARTICLE{5342440,
author={Harman, M. and McMinn, P.},
journal={Software Engineering, IEEE Transactions on},
title={A Theoretical and Empirical Study of Search-Based Testing: Local, Global, and Hybrid Search},
year={2010},
volume={36},
number={2},
pages={226-247},
abstract={Search-based optimization techniques have been applied to structural software test data generation since 1992, with a recent upsurge in interest and activity within this area. However, despite the large number of recent studies on the applicability of different search-based optimization approaches, there has been very little theoretical analysis of the types of testing problem for which these techniques are well suited. There are also few empirical studies that present results for larger programs. This paper presents a theoretical exploration of the most widely studied approach, the global search technique embodied by Genetic Algorithms. It also presents results from a large empirical study that compares the behavior of both global and local search-based optimization on real-world programs. The results of this study reveal that cases exist of test data generation problem that suit each algorithm, thereby suggesting that a hybrid global-local search (a Memetic Algorithm) may be appropriate. The paper presents a Memetic Algorithm along with further empirical results studying its performance.},
keywords={automatic test software;genetic algorithms;program testing;search problems;genetic algorithms;hybrid global-local search problem;memetic algorithm;real-world programs;search based optimization techniques;search based testing;structural software test data generation;Automated test data generation;Evolutionary Testing;Genetic Algorithms;Hill Climbing;Royal Road;algorithms;and search;artificial intelligence;control methods;experimentation;heuristic methods;measurement;performance;problem solving;schema theory;search-based software engineering;search-based testing;testing and debugging;testing tools;theory.},
doi={10.1109/TSE.2009.71},
ISSN={0098-5589},}

@article{Harman2001833,
title = "Search-based software engineering ",
journal = "Information and Software Technology ",
volume = "43",
number = "14",
pages = "833 - 839",
year = "2001",
note = "",
issn = "0950-5849",
doi = "http://dx.doi.org/10.1016/S0950-5849(01)00189-6",
url = "http://www.sciencedirect.com/science/article/pii/S0950584901001896",
author = "Mark Harman and Bryan F Jones",
keywords = "Software engineering",
keywords = "Metaheuristic",
keywords = "Genetic algorithm ",
abstract = "This paper claims that a new field of software engineering research and practice is emerging: search-based software engineering. The paper argues that software engineering is ideal for the application of metaheuristic search techniques, such as genetic algorithms, simulated annealing and tabu search. Such search-based techniques could provide solutions to the difficult problems of balancing competing (and some times inconsistent) constraints and may suggest ways of finding acceptable solutions in situations where perfect solutions are either theoretically impossible or practically infeasible. In order to develop the field of search-based software engineering, a reformulation of classic software engineering problems as search problems is required. The paper briefly sets out key ingredients for successful reformulation and evaluation criteria for search-based software engineering. "
}
SBSE
introduction to the new field Search-Based Software Engineering. Mark's paper in 2001, the foundation of SBSE

@inproceedings{Harman:2012:GCC:2351676.2351678,
 author = {Harman, Mark and Langdon, William B. and Jia, Yue and White, David R. and Arcuri, Andrea and Clark, John A.},
 title = {The GISMOE Challenge: Constructing the Pareto Program Surface Using Genetic Programming to Find Better Programs (Keynote Paper)},
 booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
 series = {ASE 2012},
 year = {2012},
 isbn = {978-1-4503-1204-2},
 location = {Essen, Germany},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2351676.2351678},
 doi = {10.1145/2351676.2351678},
 acmid = {2351678},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Compilation, Genetic Programming, Non-functional Properties, Pareto Surface, SBSE, Search Based Optimization},
}
the GISMOE agenda. Mark's paper

@inproceedings{Fraser:2012:SEE:2337223.2337245,
 author = {Fraser, Gordon and Arcuri, Andrea},
 title = {Sound Empirical Evidence in Software Testing},
 booktitle = {Proceedings of the 2012 International Conference on Software Engineering},
 series = {ICSE 2012},
 year = {2012},
 isbn = {978-1-4673-1067-3},
 location = {Zurich, Switzerland},
 pages = {178--188},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=2337223.2337245},
 acmid = {2337245},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}
my reading group paper. use empirical study to illustrate how important the selection of programs under test is in test data generation experiments.

@INPROCEEDINGS{5635150,
author={Lakhotia, K. and Harman, M. and Gross, H.},
booktitle={Search Based Software Engineering (SSBSE), 2010 Second International Symposium on},
title={AUSTIN: A Tool for Search Based Software Testing for the C Language and Its Evaluation on Deployed Automotive Systems},
year={2010},
pages={101-110},
abstract={Despite the large number of publications on Search--Based Software Testing (SBST), there remain few publicly available tools. This paper introduces AUSTIN, a publicly available SBST tool for the C language. The paper validates the tool with an empirical study of its effectiveness and efficiency in achieving branch coverage compared to random testing and the Evolutionary Testing Framework (ETF), which is used in-house by Daimler and others for Evolutionary Testing. The programs used in the study consist of eight non--trivial, real-world C functions drawn from three embedded automotive software modules. For the majority of the functions, AUSTIN is at least as effective (in terms of achieved branch coverage) as the ETF, and is considerably more efficient.},
keywords={C language;evolutionary computation;program testing;search problems;AUSTIN;C language;SBST tool;deployed automotive systems;embedded automotive software modules;evolutionary testing framework;random testing;search based software testing;Automotive engineering;Concrete;Indexes;Input variables;Search problems;Testing;SBSE;SBST;Software Testing},
doi={10.1109/SSBSE.2010.21},}
AUSTIN, test data generation
a paper introducing the tool AUSTIN. Kiren's paper

@article{Lakhotia20102379,
title = "An empirical investigation into branch coverage for C programs using \{CUTE\} and \{AUSTIN\} ",
journal = "Journal of Systems and Software ",
volume = "83",
number = "12",
pages = "2379 - 2391",
year = "2010",
note = "<ce:title>TAIC \{PART\} 2009 - Testing: Academic &amp; Industrial Conference - Practice And Research Techniques</ce:title> ",
issn = "0164-1212",
doi = "http://dx.doi.org/10.1016/j.jss.2010.07.026",
url = "http://www.sciencedirect.com/science/article/pii/S0164121210001871",
author = "Kiran Lakhotia and Phil McMinn and Mark Harman",
keywords = "Automated test data generation",
keywords = "Search based testing",
keywords = "Concolic testing",
keywords = "Symbolic execution ",
abstract = "Automated test data generation has remained a topic of considerable interest for several decades because it lies at the heart of attempts to automate the process of Software Testing. This paper reports the results of an empirical study using the dynamic symbolic-execution tool, CUTE, and a search based tool, \{AUSTIN\} on five non-trivial open source applications. The aim is to provide practitioners with an assessment of what can be achieved by existing techniques with little or no specialist knowledge and to provide researchers with baseline data against which to measure subsequent work. To achieve this, each tool is applied ‘as is’, with neither additional tuning nor supporting harnesses and with no adjustments applied to the subject programs under test. The mere fact that these tools can be applied ‘out of the box’ in this manner reflects the growing maturity of Automated test data generation. However, as might be expected, the study reveals opportunities for improvement and suggests ways to hybridize these two approaches that have hitherto been developed entirely independently. "
}
Branch Coverage, Empirical
Kiran's paper

@INPROCEEDINGS{5381642,
author={Lakhotia, K. and McMinn, P. and Harman, M.},
booktitle={Testing: Academic and Industrial Conference - Practice and Research Techniques, 2009. TAIC PART '09.},
title={Automated Test Data Generation for Coverage: Haven't We Solved This Problem Yet?},
year={2009},
pages={95-104},
abstract={Whilst there is much evidence that both concolic and search based testing can outperform random testing, there has been little work demonstrating the effectiveness of either technique with complete real world software applications. As a consequence, many researchers have doubts not only about the scalability of both approaches but also their applicability to production code. This paper performs an empirical study applying a concolic tool, CUTE, and a search based tool, AUSTIN, to the source code of four large open source applications. Each tool is applied `out of the box'; that is without writing additional code for special handling of any of the individual subjects, or by tuning the tools' parameters. Perhaps surprisingly, the results show that both tools can only obtain at best a modest level of code coverage. Several challenges remain for improving automated test data generators in order to achieve higher levels of code coverage.},
keywords={program testing;AUSTIN;CUTE;automated test data generation;concolic based testing;random testing;search based testing;Application software;Automatic testing;Automation;Concrete;Educational institutions;Open source software;Production;Scalability;Software testing;Writing;Automated test data generation;concolic testing;search based testing},
doi={10.1109/TAICPART.2009.15},}
almost identical to Kiren's paper \cite{Lakhotia20102379} "An empirical investigation into branch coverage for C programs using \{CUTE\} and \{AUSTIN\} "

@ARTICLE{5710949,
author={McMinn, P. and Harman, M. and Lakhotia, K. and Hassoun, Y. and Wegener, J.},
journal={Software Engineering, IEEE Transactions on},
title={Input Domain Reduction through Irrelevant Variable Removal and Its Effect on Local, Global, and Hybrid Search-Based Structural Test Data Generation},
year={2012},
volume={38},
number={2},
pages={453-477},
abstract={Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are “fit for purpose,” guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to open source and industrial production code. The results provide evidence to support the claim that input domain reduction has a significant effect on the performance of local, global, and hybrid search, while a purely random search is unaffected.},
keywords={automatic test pattern generation;optimisation;program compilers;program slicing;program testing;public domain software;search problems;fitness functions;hybrid search-based structural test data generation;industrial production code;input domain reduction;irrelevant variable removal;key determining factor;open source approach;program slicing;search space reduction;search-based optimization algorithm;search-based software testing;static dependence analysis;test input generation;Algorithm design and analysis;Input variables;Optimization;Search problems;Software algorithms;Software testing;Search-based software testing;automated test data generation;evolutionary testing;input domain reduction.},
doi={10.1109/TSE.2011.18},
ISSN={0098-5589},}
Test data generation, input domain reduction

@article{white2011evolutionary,
  title={Evolutionary improvement of programs},
  author={White, David R and Arcuri, Andrea and Clark, John A},
  journal={Evolutionary Computation, IEEE Transactions on},
  volume={15},
  number={4},
  pages={515--538},
  year={2011},
  publisher={IEEE}
}
evolutionary improvement
nothing much. just tells how to use evolutionary method to imporve a program.

@book{poli2008field,
  title={A field guide to genetic programming},
  author={Poli, Riccardo and Langdon, W William B and McPhee, Nicholas F and Koza, John R},
  year={2008},
  publisher={Lulu. com}
}
Genetic programming, introduction
introduction to Genetic Programing. Bill's book

@ARTICLE{996017,
author={Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
journal={Evolutionary Computation, IEEE Transactions on},
title={A fast and elitist multiobjective genetic algorithm: {NSGA-II}},
year={2002},
volume={6},
number={2},
pages={182-197},
abstract={Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN3) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed},
keywords={Pareto distribution;computational complexity;constraint theory;convergence;genetic algorithms;operations research;simulation;sorting;NSGA-II;Nondominated Sorting Genetic Algorithm II;Pareto-archived evolution strategy;Pareto-optimal front;algorithm performance;computational complexity;constrained multi-objective problems;constraint handling;convergence;dominance definition;fast elitist multi-objective genetic algorithm;mating pool;multi-criterion decision making;multi-objective evolutionary algorithm;multi-objective optimization;nondominated sharing;nonlinear problem;objectives;parent/offspring population combination;population size;selection operator;simulation;solution fitness;solution spread;strength-Pareto evolutionary algorithm;Associate members;Computational complexity;Computational modeling;Constraint optimization;Decision making;Diversity reception;Evolutionary computation;Genetic algorithms;Sorting;Testing},
doi={10.1109/4235.996017},
ISSN={1089-778X},}
NSGA II
The first paper describing NSGA-II

@inproceedings{Worm:2013:PGE:2463372.2463486,
 author = {Worm, Tony and Chiu, Kenneth},
 title = {Prioritized Grammar Enumeration: Symbolic Regression by Dynamic Programming},
 booktitle = {Proceeding of the Fifteenth Annual Conference on Genetic and Evolutionary Computation Conference},
 series = {GECCO '13},
 year = {2013},
 isbn = {978-1-4503-1963-8},
 location = {Amsterdam, The Netherlands},
 pages = {1021--1028},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2463372.2463486},
 doi = {10.1145/2463372.2463486},
 acmid = {2463486},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {genetic programming, grammar enumeration, representation, symbolic regression},
}
GP, deterministic
Tony's paper, as well as my second reading group paper. Introduced a deterministic Genetic Programming method.

@article{langdon2012genetically,
  title={Genetically Improving 50000 Lines of C++},
  author={Langdon, William B and Harman, Mark},
  journal={RN},
  volume={12},
  number={09},
  pages={09},
  year={2012}
}
Genetically Improving 50000 Lines of C++
used GP to improve a 50000 lines program(in the aspect of speed), and had a massive acceleration. Bill's paper

@article{hutter2009paramils,
  title={ParamILS: an automatic algorithm configuration framework},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin and St{\"u}tzle, Thomas},
  journal={Journal of Artificial Intelligence Research},
  volume={36},
  number={1},
  pages={267--306},
  year={2009}
}

@article{Hoffmann:2011:DKR:1961296.1950390,
 author = {Hoffmann, Henry and Sidiroglou, Stelios and Carbin, Michael and Misailovic, Sasa and Agarwal, Anant and Rinard, Martin},
 title = {Dynamic Knobs for Responsive Power-aware Computing},
 journal = {SIGPLAN Not.},
 issue_date = {March 2011},
 volume = {46},
 number = {3},
 month = mar,
 year = {2011},
 issn = {0362-1340},
 pages = {199--212},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1961296.1950390},
 doi = {10.1145/1961296.1950390},
 acmid = {1950390},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accuracy-aware computing, power-aware computing, self-aware systems},
}
knobs, parameter tuning
presents PowerDial, a system for dynamically adapting application behaviour by controlling some parameters which are imported at command line. First determine which parameters should be turned into dynamic knobs. Second run the application with each possible value of the knobs and record the behavior and resource consumption, so to obtain the Pareto front between accuracy and consumption. Then dynamically change the value of knobs on runtime.

@INPROCEEDINGS{6227211,
author={Le Goues, C. and Dewey-Vogt, M. and Forrest, S. and Weimer, W.},
booktitle={Software Engineering (ICSE), 2012 34th International Conference on},
title={A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each},
year={2012},
pages={3-13},
abstract={There are more bugs in real-world programs than human programmers can realistically address. This paper evaluates two research questions: “What fraction of bugs can be repaired automatically?” and “How much does it cost to repair a bug automatically?” In previous work, we presented GenProg, which uses genetic programming to repair defects in off-the-shelf C programs. To answer these questions, we: (1) propose novel algorithmic improvements to GenProg that allow it to scale to large programs and find repairs 68% more often, (2) exploit GenProg's inherent parallelism using cloud computing resources to provide grounded, human-competitive cost measurements, and (3) generate a large, indicative benchmark set to use for systematic evaluations. We evaluate GenProg on 105 defects from 8 open-source programs totaling 5.1 million lines of code and involving 10,193 test cases. GenProg automatically repairs 55 of those 105 defects. To our knowledge, this evaluation is the largest available of its kind, and is often two orders of magnitude larger than previous work in terms of code or test suite size or defect count. Public cloud computing prices allow our 105 runs to be reproduced for $403; a successful repair completes in 96 minutes and costs $7.32, on average.},
keywords={C language;cloud computing;genetic algorithms;program debugging;public domain software;software cost estimation;software maintenance;GenProg;algorithmic improvement;automated program repair;cloud computing resource;defect repair;genetic programming;grounded human-competitive cost measurement;off-the-shelf C program;open-source program;program bug;real-world program;repair cost;systematic evaluation;Benchmark testing;Cloud computing;Computer bugs;Genetic programming;Maintenance engineering;Open source software;Systematics;automated program repair;cloud computing;genetic programming},
doi={10.1109/ICSE.2012.6227211},
ISSN={0270-5257},}
$use genetic programming to automatic repair bugs in large programs. the representation is patch to the AST, mutation can be insertion, deletion and replacement, and crossover is a novel approach: append one parent to the other(note both are patches), and independantly delete each gene by 50:50 chance. use bug localization to minimized the scope of potential modification. use known passed and failed testcases to determine the fitness, use all negative and sampled positive testcases for each evaluation. Wes' paper

@article{Necula:2002:CTR:565816.503286,
 author = {Necula, George C. and McPeak, Scott and Weimer, Westley},
 title = {CCured: Type-safe Retrofitting of Legacy Code},
 journal = {SIGPLAN Not.},
 issue_date = {Jan. 2002},
 volume = {37},
 number = {1},
 month = jan,
 year = {2002},
 issn = {0362-1340},
 pages = {128--139},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/565816.503286},
 doi = {10.1145/565816.503286},
 acmid = {503286},
 publisher = {ACM},
 address = {New York, NY, USA},
}
proposed a type-safe language and implement an automatic transformation process from C to CCured language. type-safe means each pointer has type information. all pointers in a program are categorized into safe(points to a single value of a certain type), sequence(points to an array of values of the same type) or dynamic(couldn't determine the type pointed to). CCured language uses off-line check and runtime check for the type compatibility.

@inproceedings{wei2010automated,
  title={Automated fixing of programs with contracts},
  author={Wei, Yi and Pei, Yu and Furia, Carlo A and Silva, Lucas S and Buchholz, Stefan and Meyer, Bertrand and Zeller, Andreas},
  booktitle={Proceedings of the 19th international symposium on Software testing and analysis},
  pages={61--72},
  year={2010},
  organization={ACM}
}

@inproceedings{saha2011fault,
  title={Fault localization for data-centric programs},
  author={Saha, Diptikalyan and Nanda, Mangala Gowri and Dhoolia, Pankaj and Nandivada, V Krishna and Sinha, Vibha and Chandra, Satish},
  booktitle={Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering},
  pages={157--167},
  year={2011},
  organization={ACM}
}

@inproceedings{jones2005empirical,
  title={Empirical evaluation of the tarantula automatic fault-localization technique},
  author={Jones, James A and Harrold, Mary Jean},
  booktitle={Proceedings of the 20th IEEE/ACM international Conference on Automated software engineering},
  pages={273--282},
  year={2005},
  organization={ACM}
}

@inproceedings{anvik2006should,
  title={Who should fix this bug?},
  author={Anvik, John and Hiew, Lyndon and Murphy, Gail C},
  booktitle={Proceedings of the 28th international conference on Software engineering},
  pages={361--370},
  year={2006},
  organization={ACM}
}

@incollection{memoryallocatorreview1995,
year={1995},
isbn={978-3-540-60368-9},
booktitle={Memory Management},
volume={986},
series={Lecture Notes in Computer Science},
editor={Baler, HenryG.},
doi={10.1007/3-540-60368-9_19},
title={Dynamic storage allocation: A survey and critical review},
url={http://dx.doi.org/10.1007/3-540-60368-9_19},
publisher={Springer Berlin Heidelberg},
author={Wilson, PaulR. and Johnstone, MarkS. and Neely, Michael and Boles, David},
pages={1-116}
}
memory, review

@article{Johnstone:1998:MFP:301589.286864,
 author = {Johnstone, Mark S. and Wilson, Paul R.},
 title = {The Memory Fragmentation Problem: Solved?},
 journal = {SIGPLAN Not.},
 issue_date = {March 1999},
 volume = {34},
 number = {3},
 month = oct,
 year = {1998},
 issn = {0362-1340},
 pages = {26--36},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301589.286864},
 doi = {10.1145/301589.286864},
 acmid = {286864},
 publisher = {ACM},
 address = {New York, NY, USA},
}
memory, best fit?

@INPROCEEDINGS{4591705,
author={Shahriar, H. and Zulkernine, M.},
booktitle={Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International},
title={Mutation-Based Testing of Buffer Overflow Vulnerabilities},
year={2008},
pages={979-984},
abstract={Buffer overflow (BOF) is one of the major vulnerabilities that leads to non-secure software. Testing an implementation for BOF vulnerabilities is challenging as the underlying reasons of buffer overflow vary widely. Moreover, the existing vulnerability testing approaches do not address the issue of generating adequate test data sets for testing BOF vulnerabilities. In this work, we apply the idea of mutation-based testing technique to generate adequate test data set for BOF vulnerabilities. Our work addresses those BOF vulnerabilities, which are related to an implementation language and its associated libraries. We apply the concept for ANSI C language and its associated libraries. We propose 12 mutation operators to force the generation of adequate test data set for BOF vulnerabilities. The proposed operators are validated by using four open source programs. The results indicate that the proposed operators are effective for testing BOF vulnerabilities.},
keywords={buffer storage;program testing;public domain software;ANSI C language;buffer overflow vulnerabilities;mutation-based testing;open source programs;vulnerability testing;ANSI standards;Application software;Buffer overflow;Computer applications;Genetic mutations;Monitoring;Performance evaluation;Runtime;Software libraries;Software testing;Buffer overflow;Mutation-based testing;Vulnerabilities},
doi={10.1109/COMPSAC.2008.123},
ISSN={0730-3157},}
memory
proposed 12 mutation operators targeting the Memory overflow

@inproceedings{Risco-Martin:2009:ODM:1569901.1570116,
 author = {Risco-Mart\'{\i}n, Jos{\'e} L. and Atienza, David and Gonzalo, Rub{\'e}n and Hidalgo, J. Ignacio},
 title = {Optimization of Dynamic Memory Managers for Embedded Systems Using Grammatical Evolution},
 booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '09},
 year = {2009},
 isbn = {978-1-60558-325-9},
 location = {Montreal, Qu\&\#233;bec, Canada},
 pages = {1609--1616},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1569901.1570116},
 doi = {10.1145/1569901.1570116},
 acmid = {1570116},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded systems design, evolutionary computation, genetic programming, grammatical evolution},
}
memory
Search the best combination of different components of an allocator, to optimize the performance on a given embedded system.

@article{RiscoMartin2011755,
title = "Simulation of high-performance memory allocators ",
journal = "Microprocessors and Microsystems ",
volume = "35",
number = "8",
pages = "755 - 765",
year = "2011",
note = "<ce:title>Design and Verification of Complex Digital Systems</ce:title> ",
issn = "0141-9331",
doi = "http://dx.doi.org/10.1016/j.micpro.2011.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S0141933111000937",
author = "Jos{\'e} L. Risco-Mart\'{\i}n and J.Manuel Colmenar and David Atienza and J.Ignacio Hidalgo",
keywords = "Dynamic memory management",
keywords = "Memory allocation",
keywords = "Embedded systems design",
keywords = "Evolutionary computation",
keywords = "Grammatical evolution ",
abstract = "For the last 30&#xa0;years, a large variety of memory allocators have been proposed. Since performance, memory usage and energy consumption of each memory allocator differs, software engineers often face difficult choices in selecting the most suitable approach for their applications. To this end, custom allocators are developed from scratch, which is a difficult and error-prone process. This issue has special impact in the field of portable consumer embedded systems, that must execute a limited amount of multimedia applications, demanding high performance and extensive memory usage at a low energy consumption. This paper presents a flexible and efficient simulator to study Dynamic Memory Managers (DMMs), a composition of one or more memory allocators. This novel approach allows programmers to simulate custom and general DMMs, which can be composed without incurring any additional runtime overhead or additional programming cost. We show that this infrastructure simplifies \{DMM\} construction, mainly because the target application does not need to be compiled every time a new \{DMM\} must be evaluated and because we propose a structured method to search and build \{DMMs\} in an object-oriented fashion. Within a search procedure, the system designer can choose the “best” allocator by simulation for a particular target application and embedded system. In our evaluation, we show that our scheme delivers better performance, less memory usage and less energy consumption than single memory allocators. "
}
memory
Same writter with Risco-Martin:2009:ODM:1569901.1570116 {Optimization of Dynamic Memory Managers for Embedded Systems Using Grammatical Evolution}. Introduced a framework to conveniently compare different memory allocator using binary instrument "Pin"?

@article{RiscoMartin2010572,
title = "A parallel evolutionary algorithm to optimize dynamic memory managers in embedded systems ",
journal = "Parallel Computing ",
volume = "36",
number = "10 - 11",
pages = "572 - 590",
year = "2010",
note = "Parallel Architectures and Bioinspired Algorithms",
issn = "0167-8191",
doi = "http://dx.doi.org/10.1016/j.parco.2010.07.001",
url = "http://www.sciencedirect.com/science/article/pii/S0167819110000955",
author = "Jos{\'e} L. Risco-Mart\'{\i}n and David Atienza and J. Manuel Colmenar and Oscar Garnica",
keywords = "Embedded systems design",
keywords = "Dynamic memory management",
keywords = "Evolutionary computation",
keywords = "Distributed simulation ",
abstract = "For the last 30 years, several dynamic memory managers (DMMs) have been proposed. Such \{DMMs\} include first fit, best fit, segregated fit and buddy systems. Since the performance, memory usage and energy consumption of each \{DMM\} differs, software engineers often face difficult choices in selecting the most suitable approach for their applications. This issue has special impact in the field of portable consumer embedded systems, that must execute a limited amount of multimedia applications (e.g., 3D games, video players, signal processing software, etc.), demanding high performance and extensive memory usage at a low energy consumption. Recently, we have developed a novel methodology based on genetic programming to automatically design custom DMMs, optimizing performance, memory usage and energy consumption. However, although this process is automatic and faster than state-of-the-art optimizations, it demands intensive computation, resulting in a time-consuming process. Thus, parallel processing can be very useful to enable to explore more solutions spending the same time, as well as to implement new algorithms. In this paper we present a novel parallel evolutionary algorithm for \{DMMs\} optimization in embedded systems, based on the Discrete Event Specification (DEVS) formalism over a Service Oriented Architecture (SOA) framework. Parallelism significantly improves the performance of the sequential exploration algorithm. On the one hand, when the number of generations are the same in both approaches, our parallel optimization framework is able to reach a speed-up of 86.40× when compared with other state-of-the-art approaches. On the other, it improves the global quality (i.e., level of performance, low memory usage and low energy consumption) of the final \{DMM\} obtained in a 36.36% with respect to two well-known general-purpose \{DMMs\} and two state-of-the-art optimization methodologies. "
}
memory
Same writter again. This time they seem to focus on the parallel evolutionary algorithm.

﻿@article {SPE:SPE4380230804,
author = {Grunwald, Dirk and Zorn, Benjamin},
title = {Customalloc: Efficient synthesized memory allocators},
journal = {Software: Practice and Experience},
volume = {23},
number = {8},
publisher = {John Wiley & Sons, Ltd.},
issn = {1097-024X},
url = {http://dx.doi.org/10.1002/spe.4380230804},
doi = {10.1002/spe.4380230804},
pages = {851--869},
keywords = {Dynamic storage allocation, Profile-based optimization, Memory management, Performance evaluation, Program customization},
year = {1993},
abstract = {The allocation and disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. Yet, in some applications, programmers use domain-specific knowledge in an attempt to improve the speed or memory utilization of memory allocators.In this paper, we describe a program (CustoMalloc) that synthesizes a memory allocator customized for a specific application. Our experiments show that the synthesized allocators are uniformly faster and more space efficient than the Berkeley UNIX allocator. Constructing a custom allocator requires little programmer effort, usually taking only a few minutes. Experience has shown that the synthesized allocators are not overly sensitive to properties of input sets and the resulting allocators are superior even to domain-specific allocators designed by programmers. Measurements show that synthesized allocators are from two to ten times faster than widely-used allocators.},
}
memory
A tool which can customize a memory allocator to a given program. It first runs the given program and records the memory allocation and deallocation pattern, find the size(s) of most allocated memory, then make a quick single-linked list for that size(s), for other sizes, it uses regular routine allocator.

@article{Berger:2001:CHM:381694.378821,
 author = {Berger, Emery D. and Zorn, Benjamin G. and McKinley, Kathryn S.},
 title = {Composing High-performance Memory Allocators},
 journal = {SIGPLAN Not.},
 issue_date = {May 2001},
 volume = {36},
 number = {5},
 month = may,
 year = {2001},
 issn = {0362-1340},
 pages = {114--124},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381694.378821},
 doi = {10.1145/381694.378821},
 acmid = {378821},
 publisher = {ACM},
 address = {New York, NY, USA},
}
memory
Implement different components of different memory management algorithms and let users to combine those components to customize their own allocator. The aim of this work is to reduce the time spent on writting custom allocator from scratch. 

@incollection{Jula2007,
year={2007},
isbn={978-3-540-72520-6},
booktitle={Languages and Compilers for Parallel Computing},
volume={4382},
series={Lecture Notes in Computer Science},
editor={Almási, George and Caşcaval, Călin and Wu, Peng},
doi={10.1007/978-3-540-72521-3_22},
title={Custom Memory Allocation for Free},
url={http://dx.doi.org/10.1007/978-3-540-72521-3_22},
publisher={Springer Berlin Heidelberg},
author={Jula, Alin and Rauchwerger, Lawrence},
pages={299-313}
}
memory
Introduce Defero, a memory allocator infrastructure, which uses container's context and maintains a tree like structure to allocate memory, in order to improve the data locality. Because high locality leads to less cache page miss hence costs less time.

@article{Berger:2002:RCM:583854.582421,
 author = {Berger, Emery D. and Zorn, Benjamin G. and McKinley, Kathryn S.},
 title = {Reconsidering Custom Memory Allocation},
 journal = {SIGPLAN Not.},
 issue_date = {November 2002},
 volume = {37},
 number = {11},
 month = nov,
 year = {2002},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/583854.582421},
 doi = {10.1145/583854.582421},
 acmid = {582421},
 publisher = {ACM},
 address = {New York, NY, USA},
}
memory
Replace the custom allocator with general purpose allocator in some benchmarks and find that custom allocator is unneccessarily better than general allocator such as DLmalloc, sometimes even worse than DLmalloc. The paper also presents a generalization of general allocator called reaps(region+heaps) which combines the region semantics into general allocator. Experiments show that reap can almost performs at least as good as custom allocator and sometimes better. For those custom allocators with better performance are region-like ones, but region method costs more memory in the exchange of time. 
The most interesting claim in this paper is that one who considers to build his own custom allocator should use general allocator like DLmalloc instead.

@inproceedings{Feng:2005:LDM:1111583.1111594,
 author = {Feng, Yi and Berger, Emery D.},
 title = {A Locality-improving Dynamic Memory Allocator},
 booktitle = {Proceedings of the 2005 Workshop on Memory System Performance},
 series = {MSP '05},
 year = {2005},
 isbn = {1-59593-147-3},
 location = {Chicago, Illinois},
 pages = {68--77},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1111583.1111594},
 doi = {10.1145/1111583.1111594},
 acmid = {1111594},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {allocator, cache locality, fragmentation, memory management, paging, vam, virtual memory},
}
memory
Introducing a page oriented memory allocator, Vam. Vam saves the overhead on small size chunks by allocating them on the same page. It aims to reduce the fragmentation while increase the locality. But it seems like there is no significant improvement.

@article{Novark:2007:EAC:1273442.1250736,
 author = {Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Exterminator: Automatically Correcting Memory Errors with High Probability},
 journal = {SIGPLAN Not.},
 issue_date = {June 2007},
 volume = {42},
 number = {6},
 month = jun,
 year = {2007},
 issn = {0362-1340},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273442.1250736},
 doi = {10.1145/1273442.1250736},
 acmid = {1250736},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dieFast, dynamic memory allocation, error correction, exterminator, memory errors, probabilistic algorithms, randomized algorithms},
}
memory, exterminator
Introduced a system, Exterminator, to automatically correct some memory errors such as memory-overflow, access-before-initialized, access-after-freed

@article{Berger:2006:DPM:1133255.1134000,
 author = {Berger, Emery D. and Zorn, Benjamin G.},
 title = {DieHard: Probabilistic Memory Safety for Unsafe Languages},
 journal = {SIGPLAN Not.},
 issue_date = {June 2006},
 volume = {41},
 number = {6},
 month = jun,
 year = {2006},
 issn = {0362-1340},
 pages = {158--168},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134000},
 doi = {10.1145/1133255.1134000},
 acmid = {1134000},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DieHard, dynamic memory allocation, probabilistic memory safety, randomization, replication},
}

@inproceedings{Jula:2009:TMA:1542431.1542447,
 author = {Jula, Alin and Rauchwerger, Lawrence},
 title = {Two Memory Allocators That Use Hints to Improve Locality},
 booktitle = {Proceedings of the 2009 International Symposium on Memory Management},
 series = {ISMM '09},
 year = {2009},
 isbn = {978-1-60558-347-1},
 location = {Dublin, Ireland},
 pages = {109--118},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1542431.1542447},
 doi = {10.1145/1542431.1542447},
 acmid = {1542447},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
memory, locality
gives two locality improving memory allocation schemes. The first one, TP(Two Partition), firstly tries to allocate a chunk close to the hint address, using a K-regions method(K-region is a chunk with size of 2^K, and the memory within the same K-region are in the same size and assumed to be close to each other). Then it allocates the memory in traditional size-based way if the first attempt fails. The second scheme, Medius, similar to TP, uses K-regions method to allocate as well. But the difference is that, the size of the memory within the same K-region could be different.

@article{Hasan20061051,
title = "A tunable hybrid memory allocator ",
journal = "Journal of Systems and Software ",
volume = "79",
number = "8",
pages = "1051 - 1063",
year = "2006",
note = "",
issn = "0164-1212",
doi = "http://dx.doi.org/10.1016/j.jss.2005.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S016412120500141X",
author = "Yusuf Hasan and J. Morris Chang",
keywords = "Dynamic memory allocation",
keywords = "Performance",
keywords = "Tuning",
keywords = "Optimization ",
abstract = "Dynamic memory management can make up to 60% of total program execution time. Object oriented languages such as C++ can use 20 times more memory than procedural languages like C. Bad memory management causes severe waste of memory, several times that actually needed, in programs. It can also cause degradation in performance. Many widely used allocators waste memory and/or \{CPU\} time. Since computer memory is an expensive and limited resource its efficient utilization is necessary. There cannot exist a memory allocator that will deliver best performance and least memory consumption for all programs and therefore easily tunable allocators are required. General purpose allocators that come with operating systems give less than optimal performance or memory consumption. An allocator with a few tunable parameters can be tailored to a program’s needs for optimal performance and memory consumption. Our tunable hybrid allocator design shows 11–54% better performance and nearly equal memory consumption when compared to the well known Doug Lea allocator in seven benchmark programs. "
}
memory, glibc
This is the memory allocator that glibc uses now. Best fit for Large request, segregated list for medium and small request, also keep a quick list for small chunks. This paper also summarizes different kinds of allocators/allocation algorithms. But no better saving in memory has been achieved.

@misc{lea1996memory,
  title={A memory allocator},
  author={Lea, Doug and Gloger, Wolfram},
  month = April,
  year={2000},
  note = {http://g.oswego.edu/dl/html/malloc.html}
}
dlmalloc

@article{Zorn:1992:EMS:142181.142200,
 author = {Zorn, Benjamin and Grunwald, Dirk},
 title = {Empirical Measurements of Six Allocation-intensive C Programs},
 journal = {SIGPLAN Not.},
 issue_date = {Dec. 1992},
 volume = {27},
 number = {12},
 month = dec,
 year = {1992},
 issn = {0362-1340},
 pages = {71--80},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/142181.142200},
 doi = {10.1145/142181.142200},
 acmid = {142200},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{SIR2005,
year={2005},
issn={1382-3256},
journal={Empirical Software Engineering},
volume={10},
number={4},
doi={10.1007/s10664-005-3861-2},
title={Supporting Controlled Experimentation with Testing Techniques: An Infrastructure and its Potential Impact},
url={http://dx.doi.org/10.1007/s10664-005-3861-2},
publisher={Kluwer Academic Publishers},
keywords={Software testing; regression testing; controlled experimentation; experiment infrastructure},
author={Do, Hyunsook and Elbaum, Sebastian and Rothermel, Gregg},
pages={405-435},
language={English}
}

@INPROCEEDINGS{4401979,
author={Hutter, Frank and Babic, Domagoj and Hoos, Holger H. and Hu, A.J.},
booktitle={Formal Methods in Computer Aided Design, 2007. FMCAD '07},
title={Boosting Verification by Automatic Tuning of Decision Procedures},
year={2007},
month={Nov},
pages={27-34},
abstract={Parameterized heuristics abound in computer aided design and verification, and manual tuning of the respective parameters is difficult and time-consuming. Very recent results from the artificial intelligence (AI) community suggest that this tuning process can be automated, and that doing so can lead to significant performance improvements; furthermore, automated parameter optimization can provide valuable guidance during the development of heuristic algorithms. In this paper, we study how such an AI approach can improve a state-of-the-art SAT solver for large, real-world bounded model-checking and software verification instances. The resulting, automatically-derived parameter settings yielded runtimes on average 4.5 times faster on bounded model checking instances and 500 times faster on software verification problems than extensive hand-tuning of the decision procedure. Furthermore, the availability of automatic tuning influenced the design of the solver, and the automatically-derived parameter settings provided a deeper insight into the properties of problem instances.},
keywords={Artificial intelligence;Boosting;Encoding;Formal verification;Hardware;Heuristic algorithms;Runtime;Support vector machine classification;Support vector machines;Testing;Boolean Satisfiability;Decision Procedures;Search Parameter Optimization},
doi={10.1109/FAMCAD.2007.9},}

@incollection{arcuri-ssbse-2011,
year={2011},
isbn={978-3-642-23715-7},
booktitle={Search Based Software Engineering},
volume={6956},
series={Lecture Notes in Computer Science},
editor={Cohen, MyraB. and \'{O} Cinn\'{e}ide, Mel},
doi={10.1007/978-3-642-23716-4_6},
title={On Parameter Tuning in Search Based Software Engineering},
url={http://dx.doi.org/10.1007/978-3-642-23716-4_6},
publisher={Springer Berlin Heidelberg},
keywords={Search based software engineering; test data generation; object-oriented; unit testing},
author={Arcuri, Andrea and Fraser, Gordon},
pages={33-47},
language={English}
}


@INPROCEEDINGS{Vuduc01statisticalmodels,
    author = {Richard Vuduc and James W. Demmel and Jeff Bilmes},
    title = {Statistical Models for Automatic Performance Tuning},
    booktitle = {In Proceedings of the 2001 International Conference on Computational Science (ICCS 2001},
    year = {2001},
    pages = {117--126}
}

@inproceedings{Whaley:1998:ATL:509058.509096,
 author = {Whaley, R. Clint and Dongarra, Jack J.},
 title = {Automatically Tuned Linear Algebra Software},
 booktitle = {Proceedings of the 1998 ACM/IEEE Conference on Supercomputing},
 series = {Supercomputing '98},
 year = {1998},
 isbn = {0-89791-984-X},
 location = {San Jose, CA},
 pages = {1--27},
 numpages = {27},
 url = {http://dl.acm.org/citation.cfm?id=509058.509096},
 acmid = {509096},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {BLAS, code generation, high performance, linear algebra, optimization, tuning},
} 

@inproceedings{Tapus:2002:AHT:762761.762771,
 author = {\c{T}\={a}pu\c{s}, Cristian and Chung, I-Hsin and Hollingsworth, Jeffrey K.},
 title = {Active Harmony: Towards Automated Performance Tuning},
 booktitle = {Proceedings of the 2002 ACM/IEEE Conference on Supercomputing},
 series = {Supercomputing '02},
 year = {2002},
 location = {Baltimore, Maryland},
 pages = {1--11},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=762761.762771},
 acmid = {762771},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@incollection{autotuning,
year={2003},
isbn={978-3-540-20359-9},
booktitle={High Performance Computing},
volume={2858},
series={Lecture Notes in Computer Science},
editor={Veidenbaum, Alex and Joe, Kazuki and Amano, Hideharu and Aiso, Hideo},
doi={10.1007/978-3-540-39707-6_11},
title={FIBER: A Generalized Framework for Auto-tuning Software},
url={http://dx.doi.org/10.1007/978-3-540-39707-6_11},
publisher={Springer Berlin Heidelberg},
keywords={Auto-Tuning; Parameter optimization; Numerical library; Performance modeling; Eigensolver},
author={Katagiri, Takahiro and Kise, Kenji and Honda, Hiroaki and Yuba, Toshitsugu},
pages={146-159}
}
@inproceedings{Harman:2007:CSF:1253532.1254729,
 author = {Harman, Mark},
 title = {The Current State and Future of Search Based Software Engineering},
 booktitle = {2007 Future of Software Engineering},
 series = {FOSE '07},
 year = {2007},
 isbn = {0-7695-2829-5},
 pages = {342--357},
 numpages = {16},
 url = {http://dx.doi.org/10.1109/FOSE.2007.29},
 doi = {10.1109/FOSE.2007.29},
 acmid = {1254729},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Brake:2008:ADS:1370018.1370031,
 author = {Brake, Nevon and Cordy, James R. and Dan Y, Elizabeth and Litoiu, Marin and Popes U, Valentina},
 title = {Automating Discovery of Software Tuning Parameters},
 booktitle = {Proceedings of the 2008 International Workshop on Software Engineering for Adaptive and Self-managing Systems},
 series = {SEAMS '08},
 year = {2008},
 isbn = {978-1-60558-037-1},
 location = {Leipzig, Germany},
 pages = {65--72},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1370018.1370031},
 doi = {10.1145/1370018.1370031},
 acmid = {1370031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autonomic computing, design recovery, rearchitecture, static analysis, tuning parameter},
} 

@INPROCEEDINGS{5477100, 
author={Schuler, D. and Zeller, A.}, 
booktitle={Software Testing, Verification and Validation (ICST), 2010 Third International Conference on}, 
title={(Un-)Covering Equivalent Mutants}, 
year={2010}, 
month={April}, 
pages={45-54}, 
keywords={Java;program testing;public domain software;Java programs;artificial defects;equivalent mutants;mutation testing measures;open source JAVALANCHE framework;program semantics;Costs;Detectors;Genetic mutations;Java;Software measurement;Software quality;Software testing;Time measurement;code coverage;dynamic analysis;mutation testing}, 
doi={10.1109/ICST.2010.30},}

@article{RiscoMartín2014109,
title = "A methodology to automatically optimize dynamic memory managers applying grammatical evolution ",
journal = "Journal of Systems and Software ",
volume = "91",
number = "0",
pages = "109 - 123",
year = "2014",
note = "",
issn = "0164-1212",
doi = "http://dx.doi.org/10.1016/j.jss.2013.12.044",
url = "http://www.sciencedirect.com/science/article/pii/S016412121400017X",
author = "Jos{\'e} L. Risco-Mart\'{\i}n and J. Manuel Colmenar and J. Ignacio Hidalgo and Juan Lanchares and Josefa D\'{\i}az",
keywords = "Genetic programming",
keywords = "Dynamic memory manager",
keywords = "Multi-objective optimization ",
abstract = "Abstract Modern consumer devices must execute multimedia applications that exhibit high resource utilization. In order to efficiently execute these applications, the dynamic memory subsystem needs to be optimized. This complex task can be tackled in two complementary ways: optimizing the application source code or designing custom dynamic memory management mechanisms. Currently, the first approach has been well established, and several automatic methodologies have been proposed. Regarding the second approach, software engineers often write custom dynamic memory managers from scratch, which is a difficult and error-prone work. This paper presents a novel way to automatically generate custom dynamic memory managers optimizing both performance and memory usage of the target application. The design space is pruned using grammatical evolution converging to the best dynamic memory manager implementation for the target application. Our methodology achieves important improvements (62.55% and 30.62% better on average in performance and memory usage, respectively) when its results are compared to five different general-purpose dynamic memory managers. "
}

@inproceedings{Colmenar:2011:MOD:2001576.2001820,
 author = {Colmenar, J. Manuel and Risco-Mart\'{\i}n, Jos{\'e} L. and Atienza, David and Hidalgo, J. Ignacio},
 title = {Multi-objective Optimization of Dynamic Memory Managers Using Grammatical Evolution},
 booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '11},
 year = {2011},
 isbn = {978-1-4503-0557-0},
 location = {Dublin, Ireland},
 pages = {1819--1826},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2001576.2001820},
 doi = {10.1145/2001576.2001820},
 acmid = {2001820},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded systems design, evolutionary computation, genetic programming, grammatical evolution},
} 

@incollection{attainment_surface:1996,
year={1996},
isbn={978-3-540-61723-5},
booktitle={Parallel Problem Solving from Nature --- PPSN IV},
volume={1141},
series={Lecture Notes in Computer Science},
editor={Voigt, Hans-Michael and Ebeling, Werner and Rechenberg, Ingo and Schwefel, Hans-Paul},
doi={10.1007/3-540-61723-X_1022},
title={On the performance assessment and comparison of stochastic multiobjective optimizers},
url={http://dx.doi.org/10.1007/3-540-61723-X_1022},
publisher={Springer Berlin Heidelberg},
author={Fonseca, CarlosM. and Fleming, PeterJ.},
pages={584-593},
language={English}
}

@ARTICLE{797969,
author={Zitzler, E. and Thiele, L.},
journal={Evolutionary Computation, IEEE Transactions on},
title={Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach},
year={1999},
month={Nov},
volume={3},
number={4},
pages={257-271},
abstract={Evolutionary algorithms (EAs) are often well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EAs are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the strength Pareto EA (SPEA), that combines several features of previous multiobjective EAs in a unique manner. It is characterized by (a) storing nondominated solutions externally in a second, continuously updated population, (b) evaluating an individual's fitness dependent on the number of external nondominated points that dominate it, (c) preserving population diversity using the Pareto dominance relationship, and (d) incorporating a clustering procedure in order to reduce the nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware-software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EAs on the 0/1 knapsack problem},
keywords={evolutionary computation;knapsack problems;optimisation;Pareto dominance relationship;clustering procedure;conflicting objectives;continuously updated population;digital hardware-software multiprocessor system;extended 0/1 knapsack problem;multiobjective evolutionary algorithms;multiobjective optimization;nondominated solutions;population diversity;strength Pareto approach;Computer aided software engineering;Computer architecture;Cost function;Evolutionary computation;Hardware;Multiprocessing systems;Pareto optimization;Sampling methods;Software systems;Space exploration},
doi={10.1109/4235.797969},
ISSN={1089-778X},}

@article{langdon2013optimising,
  title={Optimising existing software with genetic programming},
  author={Langdon, William B and Harman, Mark},
  journal={IEEE Transactions on Evolutionary Computation},
  year={2013}
}

@incollection{geneticimprovementJP,
year={2013},
isbn={978-3-642-39741-7},
booktitle={Search Based Software Engineering},
volume={8084},
series={Lecture Notes in Computer Science},
editor={Ruhe, Günther and Zhang, Yuanyuan},
doi={10.1007/978-3-642-39742-4_21},
title={Applying Genetic Improvement to MiniSAT},
url={http://dx.doi.org/10.1007/978-3-642-39742-4_21},
publisher={Springer Berlin Heidelberg},
keywords={Genetic Improvement; GISMOE; SAT},
author={Petke, Justyna and Langdon, WilliamB. and Harman, Mark},
pages={257-262},
language={English}
}

@article{Berger:2006:DPM:1133255.1134000,
 author = {Berger, Emery D. and Zorn, Benjamin G.},
 title = {DieHard: Probabilistic Memory Safety for Unsafe Languages},
 journal = {SIGPLAN Not.},
 issue_date = {June 2006},
 volume = {41},
 number = {6},
 month = jun,
 year = {2006},
 issn = {0362-1340},
 pages = {158--168},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1133255.1134000},
 doi = {10.1145/1133255.1134000},
 acmid = {1134000},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DieHard, dynamic memory allocation, probabilistic memory safety, randomization, replication},
}

@incollection{justyna2013,
year={2013},
isbn={978-3-642-39741-7},
booktitle={Search Based Software Engineering},
volume={8084},
series={Lecture Notes in Computer Science},
editor={Ruhe, Günther and Zhang, Yuanyuan},
doi={10.1007/978-3-642-39742-4_21},
title={Applying Genetic Improvement to MiniSAT},
url={http://dx.doi.org/10.1007/978-3-642-39742-4_21},
publisher={Springer Berlin Heidelberg},
keywords={Genetic Improvement; GISMOE; SAT},
author={Petke, Justyna and Langdon, WilliamB. and Harman, Mark},
pages={257-262},
language={English}
}

@inproceedings{Langdon:2014:IMI:2576768.2598244,
 author = {Langdon, William B. and Modat, Marc and Petke, Justyna and Harman, Mark},
 title = {Improving 3D Medical Image Registration CUDA Software with Genetic Programming},
 booktitle = {Proceedings of the 2014 Conference on Genetic and Evolutionary Computation},
 series = {GECCO '14},
 year = {2014},
 isbn = {978-1-4503-2662-9},
 location = {Vancouver, BC, Canada},
 pages = {951--958},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2576768.2598244},
 doi = {10.1145/2576768.2598244},
 acmid = {2598244},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, SBSE, medicine, software engineering},
} 
