\section{Related Work}

State-of-art dynamic memory managers (DMM) usually combine several
different allocation strategies to serve memory request with different
sizes. Risco-Mart\'{\i}n \emph{et al.}~\cite{RiscoMart√≠n2014109,
Colmenar:2011:MOD:2001576.2001820} search for the best allocation strategy
for different sizes as well searching for the best range of sizes on which
each strategy should be applied. 
% They first record all the memory requests
% of a subject under optimisation as the test cases to evaluate a candidate
% DMM. Then they use Grammatical Evolution to search for the separators for
% multiple size ranges and the best allocation strategy applied on each range
% at the same time. 
Their work requires human effort to implement many
allocation strategies. In our approach, changing the parameters not
only indirectly changes the separators of size ranges and allocation
strategy applied on each range, but also influences strategy behavior. 

%Some embedded systems, especially those executing multimedia applications, suffer from massive memory usage and limited resources. Risco-Martin et al.\cite{Risco-Martin:2009:ODM:1569901.1570116}\cite{RiscoMartin2010572} decomposes memory allocators into several components, for each of which there are several optional implementations of different allocation strategies. Combining different implementations to generate the optimal dynamic memory manager (DMM) becomes a searching problem. They use grammatical evolution to solve this optimization problem with two real world applications: Physics3D and VDrift. Other than \emph{dlmalloc}, they target the DMM on embedded systems that usually run memory-intensive applications. In their approach, they try to find the best combination of several basic strategies, different from which, we start from the state-of-the-art combination of allocation strategies and adjust its configuration to each application. 

%Grunwald and Zorn introduced \emph{CustoMalloc}, a system that customizes and synthesizes a memory allocator for a given application\cite{SPE:SPE4380230804}. The basic idea is, run an application and record all the memory allocation and deallocation during the run so that \emph{CustoMalloc} can find the most frequent sizes. Then the system generates a custom memory allocator using two allocation strategies for different sizes: fast but more overheaded way for the most frequent sizes and traditional way for other sizes. They also reported that the performance of a synthesized allocator is not sensitive to the input of the application, suggesting that for a given application, the memory allocation and deallocation patterns for different inputs are similar. In their and our works, we both try to create a custom memory allocator for each specific application but we use a simpler way, parameter tuning in one of the best allocators. Extending from that, we expose more valuable parameters from the source code and optimize them.

There have been many works studying the influence of algorithms'
configurations or automatically adjusting them. For purposes of comparison
and context, we focus on \emph{ParamILS} \cite{hutter2009paramils}.
\emph{ParamILS} is an automatic framework proposed by Hutter \emph{et al.},
which automatically configures an algorithm's parameters to optimise
performance on a given test suite. 
% It uses a local search and computes
% fitness by running the application with each candidate configuration. 
While
targeting parameter tuning as well, our approach focuses on standard
library code, based on the assumption that the general-purpose memory
allocators may not be optimal for each specific application. In addition,
\emph{ParamILS} can only optimize existing (shallow) parameters while our
approach expose additional parameters and adjust their values to gain more
improvement.

Hoffmann \emph{et al.} \cite{Hoffmann:2011:DKR:1950365.1950390} proposed
\emph{PowerDial}, a system which dynamically adjusts an application's
behavior to make it adaptable to fluctuating workloads. 
% It first transforms
% some configuration parameters to non-constant variables residing in the
% application's memory, so the behavior of the application can be altered by
% controlling these variables at runtime. Then it pre-runs the application
% with each possible configuration to determine how these parameters
% influence the application and memoizes the Pareto-best candidates in terms
% of application's non-functional properties and the quality of the output.
Whenever \emph{PowerDial} detects a resource shortage it sacrifices some of
output quality by changing the values of variables, allowing
the application to survive the crisis. One limitation of
this work is that the search space of configuration variables must be small
enough to admit an exhaustive search. In our work the search space is too
large to use such an exhaustive search, and thus we applied Search-Based
techniques. % Similar to \emph{ParamILS}, 
\emph{PowerDial} only operates
on existing (shallow) parameters. 

Hutter \emph{et al.} \cite{4401979} have tuned the parameters of a SAT
solver, SPEAR, by adjusting not only the explicit parameters but also many
implicit parameters. They expose almost all possibly tunable variables 
and
% and
%alternatives of heuristics as parameters and give them different degree of
%attention, weighted by human intuition. Their approach involves many tunable
% parameters and 
thus a much larger search space than ours. 
% unscalable computation efforts. 
To reduce the
computation effort by limiting the search space, we use a mutation-based
technique to find the most influential parts of the code in our work and fucos
on the adjustment of them. 
This sensitivity analysis effectively reduces the 
space, admitting practical searches.

In previous work, Software Tuning Panel for Autonomic Control (STAC)
\cite{Brake:2008:ADS:1370018.1370031} automated the expsoure of a
limited form of `deep' parameters. 
% STAC first generates a design graph for
% a subject under optimisation. The design graph represents data reference
% transition flows in the subject. It then uses the reference patterns of
% shallow parameters to discover deep parameters, whose reference pattern is
% the same as one of the shallow parameter reference patterns. 
Although STAC
can discover some deep parameters effectively, it suffers from two
limitations. First, STAC requires initial human effort to characterise
shallow parameters. Second, STAC can only find a subset of deep parameters,
those that have similar data transition patterns to the known shallow
parameters. To overcome these limitations, we apply a mutation-based
sensitivity analysis to fully automated the process of locating potential
deep parameters and subsequently apply NSGA II to search for optimal values
for these parameters to balance non-functional properties of interest. 
