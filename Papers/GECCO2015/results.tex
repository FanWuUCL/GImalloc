
\section{Results}
\label{sec_results}

% FIXME: If you can, put a paragraph summarizing this section here so that
% the section heading is not immediately followed by the subsection heading.
% Fan wrote the following.

We formalise the metrics we use to compare multi-objective optimisation approaches in this section.
The results are presented in Section \ref{sec_answers}, and are used to answer the \textbf{RQ}s.

\subsection{Metrics}
\label{sec_matrics}

To investigate RQ1 and RQ2, we collect the non-dominated set of solutions from each algorithm for 20 runs, and report it in an attainment surface as introduced by Fonseca \cite{attainment_surface:1996}. To quantitatively compare the quality of each algorithm, we calculate Hypervolume and Contribution indicators to assess the multi-objective Pareto Front.

\textbf{Hypervolume}: The Hypervolume indicator \cite{797969} measures the space dominated by a the solutions. It is defined as the hypervolume of the union of hypercubes dominated by each solution on the Front. The bigger the Hypervolume, the more area is dominated by the Pareto Front in the objective space, and thus the better the solution.

\textbf{Contribution}: Since there is no way to know the true Pareto Front, we use the non-dominated set of joint solutions from all experiments to approximate the true Pareto Front. In this case, the Contribution indicator represents the ratio of solutions on the true Pareto Front are found by a given algorithm. A higher ratio indicates a more successful search. 

To admit comparison across subject programs, objectives are normalised to the original performance of each subject.

\subsection{Answers to RQs}
\label{sec_answers}

\newcommand{\shallow}{Sha}
\newcommand{\all}{All}
\newcommand{\randomsearch}{Rand}
\newcommand{\nsgaii}{NSGA}
\newcommand{\sr}{\emph{\shallow\randomsearch}}
\newcommand{\sn}{\emph{\shallow\nsgaii}}
\newcommand{\dr}{\emph{\all\randomsearch}}
\newcommand{\dn}{\emph{\all\nsgaii}}

For brevity we use \emph{\shallow} to refer to shallow parameters and \emph{\all} to refer to all parameters including shallow and deep parameters, followed by \emph{\randomsearch} or \emph{\nsgaii} to indicate the search method used (random search or NSGA II). For example, \sn{} refers to using NSGA II to search for better values for shallow parameters.
%In subsequent graphs, the performance of the original program always locates at (1, 1) since all the performance is normalised to it.

\begin{figure*}[htb]
	\centering
	\subfigure[espresso]{
		\label{fig_attainment_espresso}
		\scalebox{0.5}{\includegraphics[width=0.47\textwidth]{espresso_attainment_best}}%espresso_shallow_random}
	}
	\subfigure[gawk]{
		\label{fig_attainment_gawk}
		\scalebox{0.5}{\includegraphics[width=0.47\textwidth]{gawk_attainment_best}}%gawk_shallow_random}
	}
	\subfigure[flex]{
		\label{fig_attainment_flex}
		\scalebox{0.5}{\includegraphics[width=0.47\textwidth]{flex_attainment_best}}%flex_shallow_random}
	}
	\subfigure[sed]{
		\label{fig_attainment_sed}
		\scalebox{0.5}{\includegraphics[width=0.47\textwidth]{sed_attainment_best}}%sed_shallow_random}
	}
	\subfigure{
		\label{fig_attainment_legend}
		\includegraphics[width=0.45\textwidth]{attainment_legend_best}
	}
	\caption{0\%-attainment surfaces of the results of \sr{}, \sn{}, \dr{}, \dn{} over 20 runs for each application.}\label{fig_attainment}
\end{figure*}

To answer RQ1 and RQ2, we first report the 0\%-attainment surfaces (the combined best solutions over all runs) of the results of \sr{}, \sn{}, \dr{} and \dn{} on all subjects in Figure \ref{fig_attainment}. The solutions are plotted according to their execution time and memory usage at the high-water-mark compared to the original performance. Specially, the original always lies at (1, 1) and pinpointed by light grey dashed lines. The high-water-mark is our primary target since the remaining non-wasted memory is needed and thus cannot be reduced. 
The figure shows that all of the algorithms can reduce time or memory consumption without reducing the other objective, implying that the default configuration of \emph{dlmalloc} is not optimal for any application considered. In three subjects (\emph{espresso}, \emph{gawk} and \emph{sed}), \dn{} outperforms the other three on memory objective. In terms of time, no algorithm is strictly better and each has its own strengths on different subjects. 
%In Figure \ref{fig_shallow_random}, we can see that Shallow algorithm is better than Random on subject \emph{sed}, while they are incomparable on the other three subjects. In Figure \ref{fig_deep_shallow}, Deep algorithm is better than Shallow on three out of four subjects: \emph{espresso}, \emph{gawk}, \emph{sed}, while incomparable on subject \emph{flex}. Notice that unlike other three subjects, Deep can not find solutions have better performance on memory consumption on subject \emph{flex}, and both Shallow and Deep algorithm perform as good as Random, it implies that the optimal solution is very easy to find in the search space. So any search algorithm would fail to ourperform Random search on this special case.

To get a closer look of the results, we calculated the Hypervolume and Contribution indicator of each algorithm on every subject, and report them in Figure \ref{fig_hypervolume} and \ref{fig_contribution} respectively in the means of boxplots for all 20 runs. 
In Figure \ref{fig_hypervolume}, all the values are normalised to the hypervolume of the `true' Pareto Front (combined non-dominating solutions from all experiments), and the closer the value is to 1 the better. It is clear that \dn{} outperforms the others on subject \emph{espresso} and \emph{sed} while it doesn't perform very well on subject \emph{flex}, and on subject \emph{gawk} the best value reached by \dn{} is better than that of the others.
%In terms of Hypervolume indicator, Deep algorithm performs the best in general, with an exception on subject \emph{flex}. Shallow algorithm is statistically better than Random on subject \emph{sed}, but there is no significant difference between them on the other three subjects. 
In terms of Contribution, the performance of all algorithms are similar to that of Hypervolume. In general \dn{} is no worse than other algorithms on all subjects but \emph{flex}, where \sn{} has the biggest Contribution value.
%At last, Figure \ref{fig_diversity} shows that, all three algorithms generate non-dominated solutions with almost the same diversity, with Shallow performs slightly better on subject \emph{flex}.

%Besides the indicators above, we are also interested in the capability of finding extreme solutions of all algorithms. This can be shown by comparing the most time-saving and memory-saving performance found by each of the algorithms. We gather the best performance in terms of time and memory respectively from all 20 runs and show how often these values can be achieved by each of the algorithms in Figure \ref{fig_best_time} and \ref{fig_best_memory}. In terms of time consumption, while incomparable on three subjects, Shallow and Deep consistantly find better performance on subject \emph{sed} than that found by Random, but there is no statistical difference between Shallow and Deep on all subjects. On the other hand, Deep algorithm is able to find more memory reduction on three out of four subjects, indicating the Deep parameters almost always carry extra information on memory usage.

Since \dn{} is good at finding better performance on memory consumption, we report the most memory-saving performance found by each algorithm of each of 20 runs in Figure \ref{fig_best_memory}. On subject \emph{espresso} and \emph{sed}, \dn{} finds more memory reduction than the other approaches. On \emph{gawk}, it does not perform as consistently, but can also find more memory reduction than other approaches in the best case. 
% FIXME: What does ``potential to do so'' mean here? Please clarify this
% analysis.
% Fan was trying to say, for gawk, though on average AllNSGA doesn't perform as good and stable as other approaches,
% but can occationally outperform other approaches.

In all experiments involving \dr{} we generated and evaluated invalid
configurations (i.e., those that that cause the program to crash). However,
this issue is not specific to our Deep Parameter approach: 
surprisingly, even by just tuning the programmer-specified shallow parameters, 
the \sr{} and \sn{} optimisations also encounter and discard some
configurations that crash the program.
Unlike shallow parameters, deep parameters are exposed from internal
sub-expressions that were not monitored or protected by the programmers,
hence they are expected to cause crashes more often. As a result, the valid
configurations are sparser in the search space. Without any guidance, \dr{}
finds valid configurations less often than \sr{}, and thus requires more
optimisation time than \sr{}. Holding the searches to the same budget means
that \dn{}, which must explore a higher search space, will exhibit a higher
variance. 
% This is also the reason why the performance of \dn{} is not alway
% stable on all subjects. 
Despite this more challenging search space, exposing and optimising deep
parameters still allows \dn{} to find better configurations
than \sn{}.
 
\begin{figure}[htbp]
	\centering
	\subfigure[espresso]{
		\label{fig_hypervolume_espresso}
		\includegraphics[width=0.22\textwidth]{espresso_hypervolume}
	}
	\subfigure[gawk]{
		\label{fig_hypervolume_gawk}
		\includegraphics[width=0.22\textwidth]{gawk_hypervolume}
	}
	\subfigure[flex]{
		\label{fig_hypervolume_flex}
		\includegraphics[width=0.22\textwidth]{flex_hypervolume}
	}
	\subfigure[sed]{
		\label{fig_hypervolume_sed}
		\includegraphics[width=0.22\textwidth]{sed_hypervolume}
	}
	\caption{Hypervolume indicator of \sr{}, \sn{}, \dr{}, \dn{} on all subjects. The values are normalised to the hypervolume of the `true' Pareto Front. Larger values are better.}\label{fig_hypervolume}
\end{figure}

\begin{figure}[htbp]
	\centering
	\subfigure[espresso]{
		\label{fig_contribution_espresso}
		\includegraphics[width=0.22\textwidth]{espresso_contribution}
	}
	\subfigure[gawk]{
		\label{fig_contribution_gawk}
		\includegraphics[width=0.22\textwidth]{gawk_contribution}
	}
	\subfigure[flex]{
		\label{fig_contribution_flex}
		\includegraphics[width=0.22\textwidth]{flex_contribution}
	}
	\subfigure[sed]{
		\label{fig_contribution_sed}
		\includegraphics[width=0.22\textwidth]{sed_contribution}
	}
	\caption{Contribution indicator of \sr{}, \sn{}, \dr{}, \dn{} on all subjects. Larger values are better.}\label{fig_contribution}
\end{figure}

\begin{figure}[htb]
	\centering
	\subfigure[espresso]{
		\label{fig_best_time_espresso}
		\includegraphics[width=0.22\textwidth]{espresso_best_memory}
	}
	\subfigure[gawk]{
		\label{fig_best_time_gawk}
		\includegraphics[width=0.22\textwidth]{gawk_best_memory}
	}
	\subfigure[flex]{
		\label{fig_best_time_flex}
		\includegraphics[width=0.22\textwidth]{flex_best_memory}
	}
	\subfigure[sed]{
		\label{fig_best_time_sed}
		\includegraphics[width=0.22\textwidth]{sed_best_memory}
	}
	\caption{The best memory reduction found by each algorithm. Smaller numbers are better.}\label{fig_best_memory}
\end{figure}

To enable a more quantitive look at maximal time and memory savings, we
examine the extreme performance observed in our experiments. We report
those have the best performance on
one objective, even at the cost of reducing performance on the other
objective, found by each
algorithm on each subject and summarise them in Table
\ref{table_best_time_memory}. Some of these results are significant
departures from the original and are thus not plotted in Figure
\ref{fig_attainment}. 

\begin{table*}[htbp]
\centering
\caption{Best reduction of time or memory (separately) found by each algorithm}
\label{table_best_time_memory}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|r|r|r|r|r|r|r|r|r|r|}
\hline
\multirow{2}{*}{Subject} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Time\\ Original (s)\end{tabular}}} & \multicolumn{4}{c|}{Time Reduction (\%)} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Memory Original\\ (Peak/Wasted KB)\end{tabular}}} & \multicolumn{4}{c|}{Wasted Memory Reduction (\%)} \\ \cline{3-6} \cline{8-11} 
                         & \multicolumn{1}{c|}{}                                                                               & \sr{}  & \sn{}  & \dr{}  & \dn{} & \multicolumn{1}{c|}{}                                                                                            & \sr{}    & \sn{}    & \dr{}    & \dn{}    \\ \hline
espresso                 & 7.24                                                                                                & 1.4      & 1.4      & 1.5     & 1.5     & 3500/521                                                                                                         & 6.1        & 6.1        & 0          & 19.2       \\ %\hline
gawk                     & 3.43                                                                                                & 3.2      & 6.7      & 4.4      & 4.4     & 29680/3552                                                                                                       & 15.6       & 15.6       & 16.2       & 20.9       \\ %\hline
flex                     & 0.13                                                                                                & 7.9      & 10.0      & 6.2      & 11.6    & 10816/525                                                                                                        & 13.0       & 13.0       & 0          & 12.2        \\ %\hline
sed                      & 0.25                                                                                                & 9.4      & 7.0      & 7.0      & 5.4     & 7048/948                                                                                                         & 3.8        & 3.8        & 2.1          & 17.9       \\ \hline
\end{tabular}
}
\end{table*}

\begin{table}[htbp]
\centering
\caption{Computation Cost in Time}
\label{table_computation_time}
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{|c|r|r|r|r|r|r|}
\hline
\multirow{2}{*}{Subject} & \multicolumn{4}{c|}{Optimisation Time (h)}                                                                        & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Exposing\\ Time (h)\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[p{2cm}]{@{}c@{}}Extra Time Needed\\ for \emph{*\nsgaii} (\%)\end{tabular}}} \\ \cline{2-5}
                         & \multicolumn{1}{c|}{\sr{}} & \multicolumn{1}{c|}{\sn{}} & \multicolumn{1}{c|}{\dr{}} & \multicolumn{1}{c|}{\dn{}} & \multicolumn{1}{c|}{}                                                                             & \multicolumn{1}{c|}{}                                                                                                           \\ \hline
espresso                 & 39.7      & 46.4     & 9.0      & 39.3     & 12.5                                                                                                    & 18.5                                                                                                                          \\ %\hline
gawk                     & 22.7      & 18.4     & 13.9     & 16.4     & 5.4                                                                                                     & 11.7                                                                                                                          \\ %\hline
flex                     & 7.7       & 6.3      & 5.3      & 5.0      & 1.3                                                                                                     & 0.7                                                                                                                           \\ %\hline
sed                      & 9.4       & 7.6      & 5.9      & 6.6      & 1.9                                                                                                     & 12.6                                                                                                                          \\ \hline
\end{tabular}
}
\end{table}


To answer RQ3, we provide the average optimisation computation time for each of the apporaches in Table \ref{table_computation_time}. Recall that \dr{} generates and evaluates numerous invalid configurations. However, since crashing or incorrect mutants can be discarded immediately, the computation time of \dr{} is the lowest among all approaches (given a fixed budget in terms of mutants considered). Similarly, \dn{} generates invalid configurations more often than \sn{}, so it costs less computation time than \sn{}. Taking the deep parameter discovery time into account, \dn{} requires slightly more time than \sn{} does, and the percentage of the extra computation time is reported in the last column of Table \ref{table_computation_time}. Ultimately, \dn{} costs at most 18\% more computation time than \sn{} (\emph{espresso}), but costs only 0.7\% more computation time on \emph{flex}, on which \dn{} does not perform as good \sn{}. Overall, since this optimisation step is a ``compile time'' rather than a run-time cost and can be done before deployment, we view the benefits of deep parameter optimisation as significantly outweighing their slight additional optimisation time cost.


