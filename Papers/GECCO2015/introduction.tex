
\section{Introduction}

% Some feel uncomfortable with "optimal" (especially if one isn't proving
% it). 
Many software systems can reap significant performance benefits from
workload- or runtime-specific configurations or optimisations.
Software developers often expose a set of parameters for users to re-configure such software systems adaptively.
However, manual parameter tuning is a demanding challenge because users are usually required not only to have extensive knowledge about the system and the workload, but also to balance many competing objectives, such as memory consumption and execution time.

Many studies have reported on the challenges of automated parameter tuning~\cite{Hoffmann:2011:DKR:1950365.1950390, Vuduc01statisticalmodels,autotuning,Whaley:1998:ATL:509058.509096,Tapus:2002:AHT:762761.762771, hutter2009paramils,arcuri-ssbse-2011}. Early work focused on finding optimal values with mathmatical approaches~\cite{Vuduc01statisticalmodels,autotuning,Whaley:1998:ATL:509058.509096,Tapus:2002:AHT:762761.762771}, while search-based software engineering (SBSE)~\cite{Harman:2007:CSF:1253532.1254729} has been used in more recent research~\cite{hutter2009paramils,arcuri-ssbse-2011, Hoffmann:2011:DKR:1950365.1950390} on this problem. Although these approaches can automatically re-configure a system, their improvements are limited to changes to existing, explicit parameters.


% The expressions detail isn't necessary here -- it's too early. 
Many software systems contain undocumented internal variables or
expressions that also affect the performance of the systems. Thus, these
elements could also be good candidates for automated parameter tuning.
However, many of these elements are `private', undocumented, or otherwise
unexposed.
% , and therefore cannot be affected by changing existing explicit
% parameters or making public API calls. 
Moreover, some internal values may
not even be stored in variables, private or otherwise, but may merely exist
as fleeting sub-expression evaluation outcomes. Identifying these variables
and expressions is very difficult for general users, as it requires a deep
understanding of the source code of the system. 


In this paper, we propose an automatic technique to discover internal variables and expressions that normally cannot be accessed directly, but impact on non-functional properties of interest. Our goal is to expose new parameters that can directly influence the values of these internal variables and expressions. To distinguish from parameters exposed by software designer (which we call ``Shallow Parameters''), we call these exposed parameters ``Deep Parameters''~\cite{Harman:2014:GIA:2593929.2600116}. Modifying shallow parameter values does not necessarily change the internal code elements represented by deep parameters. Therefore deep parameters provide additional opportunities for subsequent automated parameter tuning.


% FIXME: Why is this paragraph in the introduction instead of the related
% work? 
% Fan wants to mension this related work since it's very similar to ours to some extent,
% and talk about the novalty of ours comparing to theirs. 
% Now it is moved to Related Work section with a few description remained.
In previous work, there has been an attempt to automate the expsoure of a limited form of `deep' parameters with the Software Tuning Panel for Autonomic Control (STAC)~\cite{Brake:2008:ADS:1370018.1370031}. However, it requires initial human effort to characterise shallow parameters and can only find a subset of deep parameters. To overcome these limitations, we apply a mutation-based sensitivity analysis to fully automated the process of locating potential deep parameters and subsequently apply NSGA II to search for optimal values for these parameters to balance non-functional properties of interest. 

In this paper, we focused on two non-functional properties, memory consumption and execution time, because they are important objectives for many applications and because they are often naturally conflicting, thereby yielding an interesting and rewarding multi-objective solution space. We illustrate the approach by re-configuring a general purpose memory allocator, \emph{dlmalloc}. We choose memory allocators because they are critical to the memory consumption of many programs and can account for up to 60\% of the total execution time in some scenarios~\cite{Zorn:1992:EMS:142181.142200}. As a result, memory optimisation is a widely studied topic~\cite{Risco-Martin:2009:ODM:1569901.1570116,RiscoMartin2010572}. We evaluate our approach using four specimen systems drawn from benchmarks for \emph{dlmalloc} and real world applications. Our approach neither touches the source code of the application itself nor requires any knowledge about the application under optimisation, instead only tuning the parameters for \emph{dlmalloc} library, making it applicable to other C applications with little effort.
% FIXME: If you're only changing dlmalloc and not the client program, we
% should really state that in the intro here as an advantage and key part
% of the technique. 

% Fan puts such a statement at the end of the paragraph above.

The paper presents evidence that deep parameter optimisation targeting \emph{dlmalloc} is an effective approach for improving whole-program non-functional properties. 
We report the results of experiments that show deep parameter optimisation competes favourably with both shallow optimisation and also default configurations. For all of our subjects, deep parameter optimisation reduces memory consumption by 21\% and execution time by 12\% in the best case. The contributions of the paper are summarised as follows:


\begin{enumerate}

\item We introduce an automated approach to discover and expose deep
parameters. The discovery of these parameters enhances search-based parameter tuning.

\item We report the results of an empirical study comparing the traditional shallow parameter tuning approach with our approach. On four applications totaling over 70,000 lines of code and guarded by over 700 tests, the results show that our approach can reduce memory usage by 21\% and execution time by 12\%, whereas shallow tuning alone achieves only a 16\% and 10\% corresponding reduction. 

\item Furthermore, we evaluate the offline optimisation-time cost of our
approach. For example, in our experiments, deep parameter tuning can
improve memory savings by 14\%, at the cost of 13\% longer 
offline optimisation time. When deep parameter tuning is not helpful, this
extra optimisation-time cost reduces to a mere 0.7\%, compared to shallow
parameter tuning.  

\end{enumerate}

%The rest of this paper is organised as follows. 
%Section 2 introduces the background of memory management and a popular general-purpose memory allocator \emph{dlmalloc}.
%Section 3 describes out adaptive deep parameter tuning approach.
%Section 4 explains the experimental methods for the empirical study, the results of which are discussed in Section 5
%Section 6 discusses related work, and the paper concludes with Section 7.


