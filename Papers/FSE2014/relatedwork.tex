\section{Related Work}

Some embedded systems, especially those executing multimedia applications, suffer from massive memory usage and limited resources. Risco-Martin et al\cite{Risco-Martin:2009:ODM:1569901.1570116}\cite{RiscoMartin2010572} decomposes memory allocators into several components, for each of which there are several optional implementations of different allocation strategies. Combining different implementations to generate the optimal dynamic memory manager (DMM) becomes a searching problem. They use grammatical evolution to solve this optimization problem with two real world applications: Physics3D and VDrift. The results show that, on average their custom DMM reduces memory accesses by 23\%, memory consumption by 38\% and energy usage by 21\%, comparing with the state-of-the-art dynamic memory managers currently used by these applications. Other than \emph{dlmalloc}, they target on the DMM on embedded systems which run memory-intensive application. In their approach, they try to find the best combination of several basic strategies, different from which, we start from the state-of-the-art combination of allocation strategies and adjust its configuration to each application. 

Grunwald and Zorn introduced \emph{CustoMalloc}, a system that customizes and synthesizes a memory allocator for a given application\cite{SPE:SPE4380230804}. The basic idea is, run an application and record all the memory allocation and deallocation during the run so that \emph{CustoMalloc} can find the most frequent sizes. Then the system generates a custom memory allocator using two allocation strategies for different sizes: fast but more overhead way for the most frequent sizes and normal way for other sizes. As the results show, the synthesized allocators are uniformly faster than the Berkeley UNIX allocator whilst being more memory efficient. They also reported that the performance of a synthesized allocator is not sensitive to the input of the application, suggesting that for a given application, the memory allocation and deallocation patterns for different inputs are similar. 

Because general-purpose memory allocators may not meet the programmer's expectation on some specific applications and writting custom memory allocators from scratch is difficult and error-prone, Berger et al\cite{Berger:2001:CHM:381694.378821} introduced an infrastructure for customizing memory allocators using C++ templates and inheritance. In this infrastructure, there are different components that are sufficient to generate a custom memory allocator for programmers to choose, so that generating a new custom memory allocator is simple and easy without any additional programming cost. The results show that the performance of the customized memory allocator is comparable to \emph{dlmalloc}, one of the best uniprocessor allocators. The contribution of this work is simplifying the process of creating a custom memory allocator and minimizing the human effort.

Since improving the locality of a memory allocator can improve the memory reference speed, there are allocators developed to do so. Jula et al\cite{Jula2007} present a container-oriented memory allocator, \emph{Defero}. In \emph{Defero}, the upper level of its allocation strategy is segregated fit. But instead of double linked list, in the lower level it uses trees to store the free chunks using the context of containers as hints. \emph{Defero} always tries to allocate a new object ``close'' to another related object to improve the memory reference locality. In order to use the semantic-rich context of C++ Standard Template Library (STL) containers, only a little modification to STL container is needed. What's more, it also provides some tunable parameters for users to customize the allocator. The results show that the applications under test perform better with \emph{Defero} than those using GNU STL allocator. They also report how the tunable parameters influence the performance of \emph{Defero}.

Another locality-improving memory allocator, \emph{Vam}, is introduced by Feng and Berger\cite{Feng:2005:LDM:1111583.1111594}. It also uses segregated fit as its upper level allocation strategy, but saves the overhead in small size chunks by allocating them on the same page. For other sizes of chunks, it applies a little more overhead to preserve their locality information. The results show that \emph{Vam} performs 4\%-8\% better than \emph{dlmalloc} on general applications.

Continuing on locality improving works, Jula et al\cite{Jula:2009:TMA:1542431.1542447} present two memory allocation schemes: \emph{Two Partition} (\emph{TP}) and \emph{Medius}. They both use K-regions method to keep the location information, which is used as a hint in the first attempt of allocation. Then they use the traditional size-based method to allocate the memory if the first attempt fails. The difference between \emph{TP} and \emph{Medius} is that, \emph{Medius} allows the chunks within the same K-region in different sizes whilst \emph{TP} doesn't. Then the authors compare \emph{TP} and \emph{Medius} with some other allocators including \emph{dlmalloc} and \emph{Defero}, then report and analyze the empirical results on 7 applications.

By combining most of the allocation strategies introduced previously, Hasan et al\cite{Hasan20061051} proposed a tunable hybrid memory allocator. Similar to \emph{dlmalloc}, Hasan's memory allocator uses two sets of allocation strategies for different sizes. For large requests, it manages a double linked list on which best fit strategy with deferred coalescing is applied. And for medium and small sizes, it uses segregated lists to manage the free chunks less than 1KB. What's more, it also uses a bitmap to track the emptiness of these segregated lists so that finding a non-empty free list is accelerated. A little different from serving medium requests, Hasan's allocator additionally keeps a quick list for small sizes, to which the freed chunks in small sizes are inserted before being coalesced or put back to segregated lists. The quick list is an unsorted single linked list and all the small chunks in it keep their in-use bit set. The idea is that the most common request sizes are 32 bytes or less\cite{Zorn:1992:EMS:142181.142200} so that keeping them in a quick list saves allocation time. This allocator also applies several different coalescing strategies in different scenarios, the details of which can be found in their paper. According to their results, their memory allocator performs 11-54\% better in terms of running time, compared with \emph{dlmalloc}, whilst maitaining nearly equal memory consumption.

Berger et al\cite{Berger:2002:RCM:583854.582421} generalize a general-purpose region-based allocator called \emph{reaps}, which combines the region semantics into general-purpose allocator. They show that their \emph{reaps} outperforms other allocators including some using region-like semantics. They also replace some custom allocators in their applications with \emph{dlmalloc} and show that most of the custom memory allocators perform no much better than \emph{dlmalloc}, and those who significantly outperform \emph{dlmalloc} are all \emph{reaps}-like allocators. They claim that, ``Our results indicate that programmers needing fast regions should use reaps, and that most programmers considering custom allocators should instead use the Lea allocator''.

Speaking of parameter tuning, there has been many works studying the influence of algorithms' configuration or automatically adjusting it, including \emph{ParamILS}\cite{hutter2009paramils}. \emph{ParamILS} is an automatic framework proposed by Hutter et al, which automatically configures an algorithm's parameters to get the best performance on a given test suite. It uses a local-search-based algorithm to look for the optimum and gets the fitness by running the application with each candidate configuration. Since long running time of an application leads to unbearable evaluation cost, \emph{ParamILS} uses a novel technique which adaptively controls the cut-off running time of each trial, to adjust the evaluation time. Their results show that, they achieved consistent performance improvements using \emph{ParamILS}.

Hoffmann and Sidiroglou et al\cite{Hoffmann:2011:DKR:1961296.1950390} proposed \emph{PowerDial}, a system which dynamically adjusts application's behavior to make it adaptable to fluctuating working load and power. It first transforms some configuration parameters to non-constant variables residing in the application's memory, so the behavior of the application can be altered by controling these variables when the application is running. Then it pre-runs the application with each possible configuration to abtain how these parameters influence the application, memorizes the Pareto-best candidates in terms of application's non-functional properties and the quality of the output. Whenever \emph{PowerDial} detects a resource shortage it sacrifices some of the output quality by changing the values of those variables according to its record, to prevent the application from crashing. After the resource crisis has passed, it automatically recover those values so that the application can go back to its original trace. The experimental resutls show that \emph{PowerDial} can enable four benchmark applications to survive power caps effectively.
